{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Weighted Entropy\n",
    "As metnioned in the previous notebook, there are different methods to handle missing values in a random forest. A problem with a lot of them is that a lot of them train on the missing data in the train set, to classify the missing data in the test set better. However, what I want to achieve is a better classification on a test set without missing data, so most of those algorithms won't work for this.\n",
    "\n",
    "During one of the thesis meeting sessions, we brainstormed which possible approach would be best. Our guess was weighted branches. However, usually this trains weights during training, and during testing, it discovers all possible branches, and creates a weighted sum of all those probabilities. This is of course not useful if there isn't any missing data in the test set.\n",
    "\n",
    "So we need to find a way to use this during training. I'll attempt this using a weighted entropy.\n",
    "\n",
    "This is the current calculate_entropy function (located in  CustomForest/decision_tree.py):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
    "    log2 = lambda x: math.log(x) / math.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        print(f\"p({label}) = {p}\")\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So for each unique label in our tree node, we calculate the relative frequncy of that label in the node (p) and multiply it with the log2 of that frequency (which is negative, since probabilities are between 0 and 1), but it's made positive again using the minus sign. We then sum all those values together, and return the entropy.\n",
    "\n",
    "Let's give an example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(0) = 0.25\n",
      "p(1) = 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8112781244591328"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1, 1, 1, 0])\n",
    "calculate_entropy(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of those labels, we now also have a weight for each of the instances and calculate the weighted entropy. All those weights start with 1, but get halved when a split has occured of which the value for the corresponding data instance is missing.\n",
    "\n",
    "If the data is random, p is lower. `p = count / len(y)`. We want to decrease the penalty (so increas p) if there are lower weihgts and increase the penality if there are higher weights. WARNING: All without mising data should be better than with missing data in the pure values.\n",
    "\n",
    "We also want lower values to have less impact on the count."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some notes when searching for a new entropy function:\n",
    "![](img/entropy_notes.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1. , 0.5],\n       [1. , 1. ],\n       [1. , 1. ],\n       [0. , 1. ]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.ones(len(y))\n",
    "\n",
    "weights[0] = 0.5\n",
    "\n",
    "np.concatenate((y.reshape(-1, 1), weights.reshape(-1, 1)), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def calculate_weighted_entropy(y, weights):\n",
    "    \"\"\" Calculate the weighted entropy of label array y \"\"\"\n",
    "    log2 = lambda x: math.log(x) / math.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    weighted_sum = np.sum(weights)\n",
    "    most_common_label = unique_labels[np.argmax([len(y[y == label]) for label in unique_labels])]\n",
    "    entropy = 0\n",
    "    entropy_text_formula = \"\"\n",
    "    for label in unique_labels:\n",
    "        label_weights = weights[y == label]\n",
    "\n",
    "        label_count = len(label_weights)\n",
    "\n",
    "        p = label_count / len(y)\n",
    "\n",
    "        if label == most_common_label:\n",
    "            weighted_count = sum(label_weights)\n",
    "            p_weighted = weighted_count / weighted_sum\n",
    "            diff = p_weighted - p\n",
    "            print(f\"Diff: {diff}\")\n",
    "            print(f\"Using weighted p ({p_weighted}) instead of p ({p})\")\n",
    "            p = p_weighted\n",
    "\n",
    "\n",
    "        label_weight = np.prod(label_weights)\n",
    "        entropy += -p * log2(p)\n",
    "        entropy_text_formula += f\"-{p} * log2({p}) (for label {label}) + \"\n",
    "    print(entropy_text_formula)\n",
    "    return entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's verify our entropy is the same if all weights are 1, our entropy is lower if the weights of the ones are lowered and higher if the weights of the zeros are lowered (a higher entropy means more random, less grouped)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: -0.0357142857142857\n",
      "Using weighted p (0.7142857142857143) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -0.7142857142857143 * log2(0.7142857142857143) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8467334479787441"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ones lowered, should be higher\n",
    "weights = np.array([0.5, 1, 1, 1])\n",
    "calculate_weighted_entropy(y, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0\n",
      "Using weighted p (0.75) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -0.75 * log2(0.75) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8112781244591328"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights all 1, should be the same\n",
    "weights = np.array([1, 1, 1, 1])\n",
    "calculate_weighted_entropy(y, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.1071428571428571\n",
      "Using weighted p (0.8571428571428571) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -0.8571428571428571 * log2(0.8571428571428571) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6906220754312411"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros lowered, should be lower\n",
    "weights = np.array([1, 1, 1, 0.5])\n",
    "calculate_weighted_entropy(y, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This seems to work as intended. Let's try another example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "labels = np.array([1, 1, 1, 0, 0, 0, 0, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0\n",
      "Using weighted p (0.625) instead of p (0.625)\n",
      "-0.625 * log2(0.625) (for label 0) + -0.375 * log2(0.375) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9544340029249649"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "calculate_weighted_entropy(labels, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: -0.025000000000000022\n",
      "Using weighted p (0.6) instead of p (0.625)\n",
      "-0.6 * log2(0.6) (for label 0) + -0.375 * log2(0.375) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9728184187292901"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([1, 1, 1, 1, 1, 1, 1, 0.5])\n",
    "calculate_weighted_entropy(labels, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: -0.03879310344827591\n",
      "Using weighted p (0.5862068965517241) instead of p (0.625)\n",
      "-0.5862068965517241 * log2(0.5862068965517241) (for label 0) + -0.375 * log2(0.375) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9823221179507028"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([1, 1, 1, 1, 1, 1, 1, 0.25])\n",
    "calculate_weighted_entropy(labels, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.04166666666666663\n",
      "Using weighted p (0.6666666666666666) instead of p (0.625)\n",
      "-0.6666666666666666 * log2(0.6666666666666666) (for label 0) + -0.375 * log2(0.375) (for label 1) + \n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9206140627103372"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([0.5, 1, 1, 1, 1, 1, 1, 1])\n",
    "calculate_weighted_entropy(labels, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a new entropy function, we need to implement the weights. In the build_tree function, they should get initialized with 1 (unless it's a subtree, then the value should be passed on). If the split occurs on a missing value, the weights should be halved.\n",
    "\n",
    "Because the split always happen on the entropy and the test set doesn't contain missing values, we won't change the evaluation function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnod\\PycharmProjects\\thesis\\CustomForest\\utils.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([X_1, X_2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0\n",
      "Using weighted p (0.75) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -0.75 * log2(0.75) (for label 1) + \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27812/754875219.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mtree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\CustomForest\\weighted_decision_tree.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    340\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_impurity_calculation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_calculate_information_gain_weighted\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    341\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_leaf_value_calculation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_majority_vote\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 342\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mWeightedClassificationTree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\CustomForest\\weighted_decision_tree.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, loss)\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[1;34m\"\"\" Build decision tree \"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mone_dim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 71\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_build_tree\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     72\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\CustomForest\\weighted_decision_tree.py\u001B[0m in \u001B[0;36m_build_tree\u001B[1;34m(self, X, y, weights, current_depth)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m                         \u001B[1;31m# Calculate impurity\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 129\u001B[1;33m                         \u001B[0mimpurity\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_impurity_calculation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcurrent_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my1_weights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my2_weights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    130\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m                         \u001B[1;31m# If this threshold resulted in a higher information gain than previously\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\CustomForest\\weighted_decision_tree.py\u001B[0m in \u001B[0;36m_calculate_information_gain_weighted\u001B[1;34m(self, y, y1, y2, y_weights, y1_weights, y2_weights)\u001B[0m\n\u001B[0;32m    321\u001B[0m         \u001B[0mentropy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalculate_weighted_entropy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_weights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    322\u001B[0m         \u001B[0minfo_gain\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mentropy\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 323\u001B[1;33m                     \u001B[0mcalculate_weighted_entropy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my1_weights\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    324\u001B[0m                     \u001B[0mcalculate_weighted_entropy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my2_weights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\CustomForest\\weighted_decision_tree.py\u001B[0m in \u001B[0;36mcalculate_weighted_entropy\u001B[1;34m(y, weights)\u001B[0m\n\u001B[0;32m    284\u001B[0m     \u001B[0mentropy_text_formula\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    285\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mlabel\u001B[0m \u001B[1;32min\u001B[0m \u001B[0munique_labels\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 286\u001B[1;33m         \u001B[0mlabel_weights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0my\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    287\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m         \u001B[0mlabel_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabel_weights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "from util import evaluate_no_cv\n",
    "from CustomForest import WeightedClassificationTree\n",
    "import numpy as np\n",
    "\n",
    "tree = WeightedClassificationTree()\n",
    "\n",
    "y = np.array([1, 1, 1, 0])\n",
    "X = np.array([[1, 1], [1, 2], [1, 3], [2, 1]])\n",
    "\n",
    "tree.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 1, 1, 1]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CustomForest import ClassificationTree\n",
    "tree = ClassificationTree()\n",
    "tree.fit(X, y)\n",
    "tree.predict([[2, 1], [1, 1], [1, 3], [2, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: [-0.08333333]\n",
      "Using weighted p ([0.66666667]) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -[0.66666667] * log2([0.66666667]) (for label 1) + \n",
      "Diff: 0.0\n",
      "Using weighted p (0.5) instead of p (0.5)\n",
      "-0.5 * log2(0.5) (for label 0.0) + -0.5 * log2(0.5) (for label 1.0) + \n",
      "Diff: 0.0\n",
      "Using weighted p (1.0) instead of p (1.0)\n",
      "-1.0 * log2(1.0) (for label 1.0) + \n",
      "Diff: [-0.08333333]\n",
      "Using weighted p ([0.66666667]) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -[0.66666667] * log2([0.66666667]) (for label 1) + \n",
      "Diff: 0.0\n",
      "Using weighted p (1.0) instead of p (1.0)\n",
      "-1.0 * log2(1.0) (for label 0.0) + \n",
      "Diff: 0.0\n",
      "Using weighted p (1.0) instead of p (1.0)\n",
      "-1.0 * log2(1.0) (for label 1.0) + \n",
      "Diff: [-0.15]\n",
      "Using weighted p ([0.6]) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -[0.6] * log2([0.6]) (for label 1) + \n",
      "Diff: -0.16666666666666663\n",
      "Using weighted p (0.5) instead of p (0.6666666666666666)\n",
      "-0.3333333333333333 * log2(0.3333333333333333) (for label 0.0) + -0.5 * log2(0.5) (for label 1.0) + \n",
      "Diff: 0.0\n",
      "Using weighted p (1.0) instead of p (1.0)\n",
      "-1.0 * log2(1.0) (for label 1.0) + \n",
      "Diff: [-0.15]\n",
      "Using weighted p ([0.6]) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -[0.6] * log2([0.6]) (for label 1) + \n",
      "Diff: 0.0\n",
      "Using weighted p (1.0) instead of p (1.0)\n",
      "-1.0 * log2(1.0) (for label 1.0) + \n",
      "Diff: 0.16666666666666663\n",
      "Using weighted p (0.6666666666666666) instead of p (0.5)\n",
      "-0.6666666666666666 * log2(0.6666666666666666) (for label 0.0) + -0.5 * log2(0.5) (for label 1.0) + \n",
      "Diff: [-0.15]\n",
      "Using weighted p ([0.6]) instead of p (0.75)\n",
      "-0.25 * log2(0.25) (for label 0) + -[0.6] * log2([0.6]) (for label 1) + \n",
      "Diff: 0.0\n",
      "Using weighted p (1.0) instead of p (1.0)\n",
      "-1.0 * log2(1.0) (for label 1.0) + \n",
      "Diff: -0.16666666666666663\n",
      "Using weighted p (0.5) instead of p (0.6666666666666666)\n",
      "-0.3333333333333333 * log2(0.3333333333333333) (for label 0.0) + -0.5 * log2(0.5) (for label 1.0) + \n",
      "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 2 has 1 dimension(s)\n",
      "X: [[2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from util import evaluate_no_cv\n",
    "from CustomForest import WeightedClassificationTree\n",
    "import numpy as np\n",
    "\n",
    "tree = WeightedClassificationTree()\n",
    "\n",
    "y = np.array([1, 1, 1, 0])\n",
    "X = np.array([[1, np.nan], [np.nan, 2], [np.nan, 3], [2, 1]])\n",
    "\n",
    "tree.fit(X, y)\n",
    "tree.predict([[2, 1], [1, 1], [1, 3], [2, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}