{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# K-Means\n",
    "One of the methods described in the paper was using K-Means. When imputing using K-Means, you take the k closest samples (based on a certain distance measure) and take the mean of the values of the features in those samples. This gives you a complete vector, which you can use your classifier on. Instead of imputation, it's also possible to use K-means immediately for predicting. You search the k-closest samples and let them vote for the class your test sample should be.\n",
    "\n",
    "## K-Means imputation\n",
    "Let's impute the missing values using K-Means and predict using a random forest classifier.\n",
    "\n",
    "Imputing must be done after get_features, because distance function often work better on numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TRBV', 'TRBJ'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_35764/953896102.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'reaction'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\util\\features.py\u001B[0m in \u001B[0;36mget_features\u001B[1;34m(df, test)\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[0mALPHA_OR_BETA\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'beta'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 227\u001B[1;33m     \u001B[0mbeta_renamed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'CDR3_beta'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBV'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBJ'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'CDR3_beta'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'CDR3'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBV'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'V'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBJ'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'J'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    228\u001B[0m     \u001B[0mbeta_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_baseline_sequence_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta_renamed\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_prefix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'beta_'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3462\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3463\u001B[0m                 \u001B[0mkey\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3464\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_listlike_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3465\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3466\u001B[0m         \u001B[1;31m# take() does not accept boolean indexers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_get_listlike_indexer\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1312\u001B[0m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_indexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reindex_non_unique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1313\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1314\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_read_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1315\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_validate_read_indexer\u001B[1;34m(self, key, indexer, axis)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1376\u001B[0m             \u001B[0mnot_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mensure_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmissing_mask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1377\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{not_found} not in index\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1378\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1379\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['TRBV', 'TRBJ'] not in index\""
     ]
    }
   ],
   "source": [
    "# We're lucky, sklearn already has a KNNImputer built in: https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from util import get_train_dataset, get_features, fix_test\n",
    "\n",
    "df = get_train_dataset()\n",
    "# df = df.sample(n=1000)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "x = get_features(df_train)\n",
    "y = df_train['reaction']\n",
    "\n",
    "x_test = get_features(df_test,test=True)\n",
    "y_test = df_test['reaction']\n",
    "\n",
    "x_test = fix_test(x_test, x.columns)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "x_imputed = imputer.fit_transform(x)\n",
    "x_test_imputed = imputer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(x_imputed, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from util import calculate_auc_and_plot\n",
    "y_pred = clf.predict_proba(x_test_imputed)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%y_pred = clf.predict(x_test)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Way to good accuracy, I noticed test=True wasn't set, so let's see what difference that makes\n",
    "# Doing this again to reset the one hot encoder (and not use the test version)\n",
    "x = get_features(df_train)\n",
    "y = df_train['reaction']\n",
    "\n",
    "\n",
    "x_test = get_features(df_test, test=True)\n",
    "y_test = df_test['reaction']\n",
    "\n",
    "x_test = fix_test(x_test, x.columns)\n",
    "\n",
    "x_test_imputed = imputer.transform(x_test)\n",
    "\n",
    "y_pred = clf.predict_proba(x_test_imputed)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Means prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputed data\n",
    "Let's try it on the imputed data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For this, we can use the KNNClassifier from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "clf.fit(x_imputed, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(x_test_imputed)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Immediate prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "df = get_train_dataset()\n",
    "# df = df.sample(n=1000)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "x = get_features(df_train)\n",
    "y = df_train['reaction']\n",
    "\n",
    "x_test = get_features(df_test, test=True)\n",
    "y_test = df_test['reaction']\n",
    "\n",
    "x_test = fix_test(x_test, x.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For the classifier to work, we need to remove the Nan's, let's try filling them with 0\n",
    "x_zero_imputed = x.fillna(0)\n",
    "x_test_zero_imputed = x_test.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(x_zero_imputed, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(x_test_zero_imputed)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's try filling them with the mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "x_mean_imputed = imp_mean.fit_transform(x)\n",
    "x_test_mean_imputed = imp_mean.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(x_mean_imputed, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(x_test_mean_imputed)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "When first converting the dataset into feature and then imputing the missing values using a KNNImputer, we get (using random forest) an AUC of 0.837 (or 0.987 when generating features without Test=True). 0.837 is sligthly worse than the AUC I get when not using imputation and filling the NaN's with 0 (AUC=0.871).\n",
    "\n",
    "The KNeigborsClassifier didn't support missing values, so I tried different ways to impute then, to classify them afterwards with the KNeigborsClassifier. Imputing using KNNImputer gave an AUC of 0.75, zeros 0.811 and the mean 0.791."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}