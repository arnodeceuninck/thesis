{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Alpha-Beta seperately\n",
    "\n",
    "Our previous model trained on the entire dataset (with both alpha and beta). Now we will train two models, one for alpha and one for beta. This will give us two prediction, which we will then average to get the final prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# andere baseline? Beta en alpha keten nu. Als baseline: 2 modellen: eentje voor beta, eentje voor alfa. Beta trainen op alpha keten, alfa\n",
    "# baseline beter werken dan clf met enkel alfa of beta? of model trainen op\n",
    "\n",
    "# nieuwe preprint: nieuwe benchmark dataset. Pieter zet op Slack."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TRBV', 'TRBJ'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24188/2088518013.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# contains both alfa and beta features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'reaction'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\thesis\\util\\features.py\u001B[0m in \u001B[0;36mget_features\u001B[1;34m(df, test)\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[0mALPHA_OR_BETA\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'beta'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 227\u001B[1;33m     \u001B[0mbeta_renamed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'CDR3_beta'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBV'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBJ'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'CDR3_beta'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'CDR3'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBV'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'V'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TRBJ'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'J'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    228\u001B[0m     \u001B[0mbeta_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_baseline_sequence_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta_renamed\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_prefix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'beta_'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3462\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3463\u001B[0m                 \u001B[0mkey\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3464\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_listlike_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3465\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3466\u001B[0m         \u001B[1;31m# take() does not accept boolean indexers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_get_listlike_indexer\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1312\u001B[0m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_indexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reindex_non_unique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1313\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1314\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_read_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1315\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_validate_read_indexer\u001B[1;34m(self, key, indexer, axis)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1376\u001B[0m             \u001B[0mnot_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mensure_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmissing_mask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1377\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{not_found} not in index\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1378\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1379\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['TRBV', 'TRBJ'] not in index\""
     ]
    }
   ],
   "source": [
    "from util import get_train_dataset, get_features, fix_test, plot_roc_curve\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = get_train_dataset()\n",
    "# df = df.sample(n=1000) # for faster debugging\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "x = get_features(train)  # contains both alfa and beta features\n",
    "y = train['reaction']\n",
    "\n",
    "# Keep only the columns starting with 'alpha_'\n",
    "x_alpha = x[x.columns[x.columns.str.startswith('alfa_')]]\n",
    "x_beta = x[x.columns[x.columns.str.startswith('beta_')]]\n",
    "\n",
    "x_test = get_features(test, test=True) # Note: Without the test=True, I'm still getting the almost perfect prediciton error\n",
    "x_test  = fix_test(x_test, x.columns)\n",
    "y_test = test['reaction']\n",
    "\n",
    "x_test_alpha = x_test[x_test.columns[x_test.columns.str.startswith('alfa_')]]\n",
    "x_test_beta = x_test[x_test.columns[x_test.columns.str.startswith('beta_')]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try a random forest classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forest doesn't support NaN values\n",
    "x_alpha = x_alpha.fillna(0)\n",
    "x_beta = x_beta.fillna(0)\n",
    "x_test_alpha = x_test_alpha.fillna(0)\n",
    "x_test_beta = x_test_beta.fillna(0)\n",
    "\n",
    "clf_alpha = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf_beta = RandomForestClassifier(n_estimators=200, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_alpha.fit(x_alpha, y)\n",
    "clf_beta.fit(x_beta, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_alpha = clf_alpha.predict_proba(x_test_alpha)[:, 1]\n",
    "y_pred_beta = clf_beta.predict_proba(x_test_beta)[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def calculate_auc_and_plot(y_test, y_pred):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plot_roc_curve(fpr, tpr, label=f'ROC curve (area = {roc_auc:.3f})', title='ROC curve')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate AUC for alpha\n",
    "calculate_auc_and_plot(y_test, y_pred_alpha) # TODO: kijken of veel alpha's zelfde score?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate AUC for beta\n",
    "calculate_auc_and_plot(y_test, y_pred_beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = (y_pred_alpha + y_pred_beta) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y_pred is predicted using predict_proba, so we need to convert it to 0/1\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(metrics.classification_report(y_test, y_pred_label))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instead of averaging the predictions, we can also take the maximum or minimum\n",
    "import numpy as np\n",
    "y_pred_max = np.maximum(y_pred_alpha, y_pred_beta)\n",
    "\n",
    "calculate_auc_and_plot(y_test, y_pred_max)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_max_label = (y_pred_max > 0.5).astype(int)\n",
    "print(metrics.classification_report(y_test, y_pred_max_label))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_max_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Or the minimum\n",
    "y_pred_min = np.minimum(y_pred_alpha, y_pred_beta)\n",
    "\n",
    "calculate_auc_and_plot(y_test, y_pred_min)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_min_label = (y_pred_min > 0.5).astype(int)\n",
    "print(metrics.classification_report(y_test, y_pred_min_label))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_min_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Or the product\n",
    "y_pred_product = y_pred_alpha * y_pred_beta\n",
    "\n",
    "calculate_auc_and_plot(y_test, y_pred_product)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_product_label = (y_pred_product > 0.5).astype(int)\n",
    "print(metrics.classification_report(y_test, y_pred_product_label))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_product_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Product and average seem to perform the best, yielding a similar result as the model trained on the entire dataset. (and better than the model trained on only alpha or beta)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Complete only\n",
    "I'm interested now in how the missing values affects those predictions. Let's see what happens if we remove the rows with missing values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from util import get_train_dataset, get_features, fix_test, plot_roc_curve\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = get_train_dataset()\n",
    "# df = df.sample(n=1000) # for faster debugging\n",
    "\n",
    "df_length = len(df)\n",
    "df = df.dropna() # Must be here and not after the get_features, since get_features might also introduce NaN values\n",
    "print(f'Number of rows removed: {df_length - len(df)} ({(df_length - len(df)) / df_length * 100:.2f}%)')\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "x = get_features(train)  # contains both alfa and beta features\n",
    "y = train['reaction']\n",
    "\n",
    "# Keep only the columns starting with 'alpha_'\n",
    "x_alpha = x[x.columns[x.columns.str.startswith('alfa_')]]\n",
    "x_beta = x[x.columns[x.columns.str.startswith('beta_')]]\n",
    "\n",
    "x_test = get_features(test, test=True) # Note: Without the test=True, I'm still getting the almost perfect prediciton error\n",
    "x_test  = fix_test(x_test, x.columns)\n",
    "y_test = test['reaction']\n",
    "\n",
    "x_test_alpha = x_test[x_test.columns[x_test.columns.str.startswith('alfa_')]]\n",
    "x_test_beta = x_test[x_test.columns[x_test.columns.str.startswith('beta_')]]\n",
    "\n",
    "# dit model hierboven ook test op zelfde testset\n",
    "# is dat omdat model beter is of omdat testset makkelijker te voorspellen?\n",
    "# literatuur induiken, algemeen missing data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove NaN's introduced by get_features\n",
    "x_alpha = x_alpha.fillna(0)\n",
    "x_beta = x_beta.fillna(0)\n",
    "x_test_alpha = x_test_alpha.fillna(0)\n",
    "x_test_beta = x_test_beta.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_alpha = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf_beta = RandomForestClassifier(n_estimators=200, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_alpha.fit(x_alpha, y)\n",
    "clf_beta.fit(x_beta, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_alpha = clf_alpha.predict_proba(x_test_alpha)[:, 1]\n",
    "y_pred_beta = clf_beta.predict_proba(x_test_beta)[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_auc_and_plot(y_test, y_pred_alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_auc_and_plot(y_test, y_pred_beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = (y_pred_alpha + y_pred_beta) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(metrics.classification_report(y_test, y_pred_label))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alpha tells us more than beta here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's also create one model using both alpha and beta features\n",
    "from util import get_train_dataset, get_features, fix_test, plot_roc_curve # calculate_auc_and_plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = get_train_dataset()\n",
    "# df = df.sample(n=1000) # for faster debugging\n",
    "df = df.dropna()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "x = get_features(train)  # contains both alfa and beta features\n",
    "y = train['reaction']\n",
    "\n",
    "x_test = get_features(test, test=True)\n",
    "x_test  = fix_test(x_test, x.columns)\n",
    "y_test = test['reaction']\n",
    "\n",
    "# Remove NaN's introduced by get_features\n",
    "x = x.fillna(0)\n",
    "x_test = x_test.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from util import calculate_auc_and_plot\n",
    "y_pred = clf.predict_proba(x_test)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "## Seperate models\n",
    "I tried generating a seperate model for the alpha part and a seperate model for the beta part. Both models used a random forest classifier (since this has been one of the best models for this) and the NaN's where filled with 0. I didn't drop any rows, so the alpha model contained a lot of rows containing only zeros.\n",
    "\n",
    "The alpha model was trained on all alpha columns, the beta model on all beta columns. Those classifiers give a probability as output. The ROC curve of the alpha model contained a straight line, which is probably because of all 0 rows. Alpha model had an AUC of 0.767, the beta model 0.794.\n",
    "\n",
    "I tried different ways of combining the results of those models. Taking the average resulted in an ROC of 0.876 (which is the same as when using one random forest model on all columns). The misclassifications where mainly false positivess. Product gave similar results. Maximum and minimum slightly worse.\n",
    "\n",
    "## Complete only\n",
    "I was also interested in the accuracy if I dropped all rows conaining NaN's (58% of the rows). I created a seperate model for the alpha columns and a seperate model for the beta columns. This resulted in an AUC of 0.981 for the alpha model, 0.887 for the beta model. The combination (avg) had an AUC of 0.983, which is slightly higher than one model trained on both alpha and beta together (0.968)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model met op nan's getraind, zien hoe dat werkt op de testsetmet nan's (wel meer data om op te trainen), vooral vraag is er een meerwaarde om nan's mee te rekenen om modellen beter te maken'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NaN's in train, not in test\n",
    "A seperate test to see whether the NaN's actually improved the model (even if the test set doesn't contain any NaN's)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from util import get_train_dataset, get_features, fix_test, plot_roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = get_train_dataset()\n",
    "# df = df.sample(n=1000) # for faster debugging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_length = len(test)\n",
    "test = test.dropna()\n",
    "print(f'Number of rows removed: {df_test_length - len(test)} ({(df_test_length - len(test)) / df_test_length * 100:.2f}%)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = get_features(train)\n",
    "y = train['reaction']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alpha and beta together"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_test = get_features(test, test=True) # Note: Without the test=True, I'm still getting the almost perfect prediciton error\n",
    "x_test  = fix_test(x_test, x.columns)\n",
    "y_test = test['reaction']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = x.fillna(0)\n",
    "x_test = x_test.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(x_test)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alpha and beta seperately"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep only the columns starting with 'alpha_'\n",
    "x_alpha = x[x.columns[x.columns.str.startswith('alfa_')]]\n",
    "x_beta = x[x.columns[x.columns.str.startswith('beta_')]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_test_alpha = x_test[x_test.columns[x_test.columns.str.startswith('alfa_')]]\n",
    "x_test_beta = x_test[x_test.columns[x_test.columns.str.startswith('beta_')]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_alpha = x_alpha.fillna(0)\n",
    "x_beta = x_beta.fillna(0)\n",
    "x_test_alpha = x_test_alpha.fillna(0)\n",
    "x_test_beta = x_test_beta.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_alpha = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf_alpha.fit(x_alpha, y)\n",
    "y_pred_alpha = clf_alpha.predict_proba(x_test_alpha)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred_alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_beta = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf_beta.fit(x_beta, y)\n",
    "y_pred_beta = clf_beta.predict_proba(x_test_beta)[:, 1]\n",
    "calculate_auc_and_plot(y_test, y_pred_beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = (y_pred_alpha + y_pred_beta) / 2\n",
    "calculate_auc_and_plot(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "print(metrics.classification_report(y_test, y_pred_label))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}